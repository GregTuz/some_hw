{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8830fe52-3192-4064-a0d7-73622ecb1350",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, StructType, IntegerType, DateType, StructField\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6d5147c-f4e2-4220-a40b-6c33c6d5cbcb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m spark\u001b[38;5;241m.\u001b[39mstop()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "107b1dfd-a73a-4bff-b568-4df16e552cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "            .builder \\\n",
    "            .appName(\"spark_demo_1\") \\\n",
    "            .config(\"spark.jars\", \"postgresql-42.7.4.jar\") \\\n",
    "            .config(\"spark.jars.packages\", \"org.apache.spark:spark-avro_2.12:3.4.1\") \\\n",
    "            .master(\"local\") \\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea0e2b93-8238-44cd-a2fe-4a577684853a",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"file:///Users/tuzhbagrigoriy/Downloads/movies.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54fc50d8-0cc5-44a3-839f-7faa9967e315",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/25 11:09:50 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "24/11/25 11:09:50 INFO SharedState: Warehouse path is 'file:/Users/tuzhbagrigoriy/reps/some_hw/python_dz/HW9/spark-warehouse'.\n",
      "24/11/25 11:09:50 INFO InMemoryFileIndex: It took 11 ms to list leaf files for 1 paths.\n",
      "24/11/25 11:09:50 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 357.1 KiB, free 366.0 MiB)\n",
      "24/11/25 11:09:51 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.2 KiB, free 365.9 MiB)\n",
      "24/11/25 11:09:51 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.8.61.21:60155 (size: 35.2 KiB, free: 366.3 MiB)\n",
      "24/11/25 11:09:51 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0\n",
      "24/11/25 11:09:51 INFO FileInputFormat: Total input files to process : 1\n",
      "24/11/25 11:09:51 INFO FileInputFormat: Total input files to process : 1\n",
      "24/11/25 11:09:51 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
      "24/11/25 11:09:51 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/11/25 11:09:51 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)\n",
      "24/11/25 11:09:51 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/11/25 11:09:51 INFO DAGScheduler: Missing parents: List()\n",
      "24/11/25 11:09:51 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/11/25 11:09:51 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.6 KiB, free 365.9 MiB)\n",
      "24/11/25 11:09:51 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.4 KiB, free 365.9 MiB)\n",
      "24/11/25 11:09:51 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.8.61.21:60155 (size: 4.4 KiB, free: 366.3 MiB)\n",
      "24/11/25 11:09:51 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585\n",
      "24/11/25 11:09:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/11/25 11:09:51 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "24/11/25 11:09:51 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.8.61.21, executor driver, partition 0, PROCESS_LOCAL, 9452 bytes) \n",
      "24/11/25 11:09:51 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "24/11/25 11:09:51 INFO BinaryFileRDD: Input split: Paths:/Users/tuzhbagrigoriy/Downloads/movies.csv:0+5073\n",
      "24/11/25 11:09:51 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1085 bytes result sent to driver\n",
      "24/11/25 11:09:51 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 175 ms on 10.8.61.21 (executor driver) (1/1)\n",
      "24/11/25 11:09:51 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "24/11/25 11:09:51 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 0,207 s\n",
      "24/11/25 11:09:51 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/11/25 11:09:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "24/11/25 11:09:51 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 0,228268 s\n",
      "24/11/25 11:09:51 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
      "24/11/25 11:09:51 INFO DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/11/25 11:09:51 INFO DAGScheduler: Final stage: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0)\n",
      "24/11/25 11:09:51 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/11/25 11:09:51 INFO DAGScheduler: Missing parents: List()\n",
      "24/11/25 11:09:51 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/11/25 11:09:51 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.1 KiB, free 365.9 MiB)\n",
      "24/11/25 11:09:51 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.4 KiB, free 365.9 MiB)\n",
      "24/11/25 11:09:51 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.8.61.21:60155 (size: 5.4 KiB, free: 366.3 MiB)\n",
      "24/11/25 11:09:51 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585\n",
      "24/11/25 11:09:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/11/25 11:09:51 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "24/11/25 11:09:51 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.8.61.21, executor driver, partition 0, PROCESS_LOCAL, 9452 bytes) \n",
      "24/11/25 11:09:51 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
      "24/11/25 11:09:51 INFO BinaryFileRDD: Input split: Paths:/Users/tuzhbagrigoriy/Downloads/movies.csv:0+5073\n",
      "24/11/25 11:09:51 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1148 bytes result sent to driver\n",
      "24/11/25 11:09:51 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 44 ms on 10.8.61.21 (executor driver) (1/1)\n",
      "24/11/25 11:09:51 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "24/11/25 11:09:51 INFO DAGScheduler: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 0,048 s\n",
      "24/11/25 11:09:51 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/11/25 11:09:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "24/11/25 11:09:51 INFO DAGScheduler: Job 1 finished: csv at NativeMethodAccessorImpl.java:0, took 0,049383 s\n",
      "24/11/25 11:09:51 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 10.8.61.21:60155 in memory (size: 5.4 KiB, free: 366.3 MiB)\n",
      "24/11/25 11:09:51 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.8.61.21:60155 in memory (size: 35.2 KiB, free: 366.3 MiB)\n",
      "24/11/25 11:09:51 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.8.61.21:60155 in memory (size: 4.4 KiB, free: 366.3 MiB)\n"
     ]
    }
   ],
   "source": [
    "df = spark \\\n",
    "    .read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"multiline\", \"true\") \\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .option(\"quote\", \"*\") \\\n",
    "    .option(\"dateFormat\", \"M/d/y\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ea8bd5e-2bae-4b64-85f7-cc821bf725e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/25 11:09:59 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/11/25 11:09:59 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/11/25 11:09:59 INFO CodeGenerator: Code generated in 85.489458 ms\n",
      "24/11/25 11:09:59 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 353.1 KiB, free 366.0 MiB)\n",
      "24/11/25 11:09:59 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 35.1 KiB, free 365.9 MiB)\n",
      "24/11/25 11:09:59 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.8.61.21:60155 (size: 35.1 KiB, free: 366.3 MiB)\n",
      "24/11/25 11:09:59 INFO SparkContext: Created broadcast 3 from showString at NativeMethodAccessorImpl.java:0\n",
      "24/11/25 11:09:59 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4199377 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/11/25 11:09:59 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "24/11/25 11:09:59 INFO DAGScheduler: Got job 2 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/11/25 11:09:59 INFO DAGScheduler: Final stage: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0)\n",
      "24/11/25 11:09:59 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/11/25 11:09:59 INFO DAGScheduler: Missing parents: List()\n",
      "24/11/25 11:09:59 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[7] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/11/25 11:09:59 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 18.3 KiB, free 365.9 MiB)\n",
      "24/11/25 11:09:59 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 8.2 KiB, free 365.9 MiB)\n",
      "24/11/25 11:09:59 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.8.61.21:60155 (size: 8.2 KiB, free: 366.3 MiB)\n",
      "24/11/25 11:09:59 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585\n",
      "24/11/25 11:09:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/11/25 11:09:59 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
      "24/11/25 11:09:59 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (10.8.61.21, executor driver, partition 0, PROCESS_LOCAL, 9925 bytes) \n",
      "24/11/25 11:09:59 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
      "24/11/25 11:09:59 INFO CodeGenerator: Code generated in 10.219916 ms\n",
      "24/11/25 11:09:59 INFO FileScanRDD: Reading File path: file:///Users/tuzhbagrigoriy/Downloads/movies.csv, range: 0-5073, partition values: [empty row]\n",
      "24/11/25 11:09:59 INFO CodeGenerator: Code generated in 11.61575 ms\n",
      "24/11/25 11:09:59 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1816 bytes result sent to driver\n",
      "24/11/25 11:09:59 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 75 ms on 10.8.61.21 (executor driver) (1/1)\n",
      "24/11/25 11:09:59 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "24/11/25 11:09:59 INFO DAGScheduler: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0) finished in 0,087 s\n",
      "24/11/25 11:09:59 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/11/25 11:09:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
      "24/11/25 11:09:59 INFO DAGScheduler: Job 2 finished: showString at NativeMethodAccessorImpl.java:0, took 0,089471 s\n",
      "24/11/25 11:09:59 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 10.8.61.21:60155 in memory (size: 8.2 KiB, free: 366.3 MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+-------+---------------------+----------------+-------------+-----------------+---------------+----+\n",
      "|                      Film|  Genre|          Lead Studio|Audience score %|Profitability|Rotten Tomatoes %|Worldwide Gross|Year|\n",
      "+--------------------------+-------+---------------------+----------------+-------------+-----------------+---------------+----+\n",
      "|Zack and Miri Make a Porno|Romance|The Weinstein Company|              70|  1.747541667|               64|        $41.94 |2008|\n",
      "+--------------------------+-------+---------------------+----------------+-------------+-----------------+---------------+----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/25 11:10:00 INFO CodeGenerator: Code generated in 10.028625 ms\n"
     ]
    }
   ],
   "source": [
    "df.show(1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd1810f0-85ea-493c-be63-01e2bedc5213",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('film_words', f.split(f.col('film'), ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "696f5aa4-cc80-4127-be31-60328392a97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+---------+---------------------+----------------+-------------+-----------------+---------------+----+------------------------------------------+\n",
      "|                              Film|    Genre|          Lead Studio|Audience score %|Profitability|Rotten Tomatoes %|Worldwide Gross|Year|                                film_words|\n",
      "+----------------------------------+---------+---------------------+----------------+-------------+-----------------+---------------+----+------------------------------------------+\n",
      "|        Zack and Miri Make a Porno|  Romance|The Weinstein Company|              70|  1.747541667|               64|        $41.94 |2008|         [Zack, and, Miri, Make, a, Porno]|\n",
      "|                   Youth in Revolt|   Comedy|The Weinstein Company|              52|         1.09|               68|        $19.62 |2010|                       [Youth, in, Revolt]|\n",
      "|You Will Meet a Tall Dark Stranger|   Comedy|          Independent|              35|  1.211818182|               43|        $26.66 |2010|[You, Will, Meet, a, Tall, Dark, Stranger]|\n",
      "|                      When in Rome|   Comedy|               Disney|              44|          0.0|               15|        $43.04 |2010|                          [When, in, Rome]|\n",
      "|             What Happens in Vegas|   Comedy|                  Fox|              72|  6.267647029|               28|       $219.37 |2008|                [What, Happens, in, Vegas]|\n",
      "|               Water For Elephants|    Drama|     20th Century Fox|              72|  3.081421053|               60|       $117.09 |2011|                   [Water, For, Elephants]|\n",
      "|                            WALL-E|Animation|               Disney|              89|  2.896019067|               96|       $521.28 |2008|                                  [WALL-E]|\n",
      "|                          Waitress|  Romance|          Independent|              67|   11.0897415|               89|        $22.18 |2007|                                [Waitress]|\n",
      "|               Waiting For Forever|  Romance|          Independent|              53|        0.005|                6|         $0.03 |2011|                   [Waiting, For, Forever]|\n",
      "|                   Valentine's Day|   Comedy|         Warner Bros.|              54|  4.184038462|               17|       $217.57 |2010|                        [Valentine's, Day]|\n",
      "+----------------------------------+---------+---------------------+----------------+-------------+-----------------+---------------+----+------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/25 11:10:09 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/11/25 11:10:09 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/11/25 11:10:09 INFO CodeGenerator: Code generated in 13.362 ms\n",
      "24/11/25 11:10:09 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 353.1 KiB, free 365.6 MiB)\n",
      "24/11/25 11:10:09 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 35.1 KiB, free 365.5 MiB)\n",
      "24/11/25 11:10:09 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.8.61.21:60155 (size: 35.1 KiB, free: 366.2 MiB)\n",
      "24/11/25 11:10:09 INFO SparkContext: Created broadcast 5 from showString at NativeMethodAccessorImpl.java:0\n",
      "24/11/25 11:10:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4199377 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/11/25 11:10:09 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "24/11/25 11:10:09 INFO DAGScheduler: Got job 3 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/11/25 11:10:09 INFO DAGScheduler: Final stage: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0)\n",
      "24/11/25 11:10:09 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/11/25 11:10:09 INFO DAGScheduler: Missing parents: List()\n",
      "24/11/25 11:10:09 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[11] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/11/25 11:10:09 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 19.8 KiB, free 365.5 MiB)\n",
      "24/11/25 11:10:09 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 8.7 KiB, free 365.5 MiB)\n",
      "24/11/25 11:10:09 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.8.61.21:60155 (size: 8.7 KiB, free: 366.2 MiB)\n",
      "24/11/25 11:10:09 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1585\n",
      "24/11/25 11:10:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/11/25 11:10:09 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
      "24/11/25 11:10:09 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (10.8.61.21, executor driver, partition 0, PROCESS_LOCAL, 9925 bytes) \n",
      "24/11/25 11:10:09 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
      "24/11/25 11:10:09 INFO CodeGenerator: Code generated in 8.523958 ms\n",
      "24/11/25 11:10:09 INFO FileScanRDD: Reading File path: file:///Users/tuzhbagrigoriy/Downloads/movies.csv, range: 0-5073, partition values: [empty row]\n",
      "24/11/25 11:10:09 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 2962 bytes result sent to driver\n",
      "24/11/25 11:10:09 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 28 ms on 10.8.61.21 (executor driver) (1/1)\n",
      "24/11/25 11:10:09 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "24/11/25 11:10:09 INFO DAGScheduler: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0) finished in 0,032 s\n",
      "24/11/25 11:10:09 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/11/25 11:10:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
      "24/11/25 11:10:09 INFO DAGScheduler: Job 3 finished: showString at NativeMethodAccessorImpl.java:0, took 0,033857 s\n",
      "24/11/25 11:10:09 INFO CodeGenerator: Code generated in 6.765875 ms\n"
     ]
    }
   ],
   "source": [
    "df.show(10,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "48db5f8e-367e-4a07-800a-f79ddd40fc40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|          film_words|\n",
      "+--------------------+\n",
      "|[Zack, and, Miri,...|\n",
      "| [Youth, in, Revolt]|\n",
      "|[You, Will, Meet,...|\n",
      "|    [When, in, Rome]|\n",
      "|[What, Happens, i...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/25 11:10:15 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/11/25 11:10:15 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/11/25 11:10:15 INFO CodeGenerator: Code generated in 10.012125 ms\n",
      "24/11/25 11:10:15 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 353.1 KiB, free 365.2 MiB)\n",
      "24/11/25 11:10:15 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 35.1 KiB, free 365.1 MiB)\n",
      "24/11/25 11:10:15 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.8.61.21:60155 (size: 35.1 KiB, free: 366.2 MiB)\n",
      "24/11/25 11:10:15 INFO SparkContext: Created broadcast 7 from showString at NativeMethodAccessorImpl.java:0\n",
      "24/11/25 11:10:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4199377 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/11/25 11:10:15 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "24/11/25 11:10:15 INFO DAGScheduler: Got job 4 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/11/25 11:10:15 INFO DAGScheduler: Final stage: ResultStage 4 (showString at NativeMethodAccessorImpl.java:0)\n",
      "24/11/25 11:10:15 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/11/25 11:10:15 INFO DAGScheduler: Missing parents: List()\n",
      "24/11/25 11:10:15 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[15] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/11/25 11:10:16 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 16.3 KiB, free 365.1 MiB)\n",
      "24/11/25 11:10:16 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 365.1 MiB)\n",
      "24/11/25 11:10:16 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.8.61.21:60155 (size: 8.0 KiB, free: 366.2 MiB)\n",
      "24/11/25 11:10:16 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1585\n",
      "24/11/25 11:10:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[15] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/11/25 11:10:16 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
      "24/11/25 11:10:16 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (10.8.61.21, executor driver, partition 0, PROCESS_LOCAL, 9925 bytes) \n",
      "24/11/25 11:10:16 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)\n",
      "24/11/25 11:10:16 INFO CodeGenerator: Code generated in 4.558375 ms\n",
      "24/11/25 11:10:16 INFO FileScanRDD: Reading File path: file:///Users/tuzhbagrigoriy/Downloads/movies.csv, range: 0-5073, partition values: [empty row]\n",
      "24/11/25 11:10:16 INFO CodeGenerator: Code generated in 3.126083 ms\n",
      "24/11/25 11:10:16 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1728 bytes result sent to driver\n",
      "24/11/25 11:10:16 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 16 ms on 10.8.61.21 (executor driver) (1/1)\n",
      "24/11/25 11:10:16 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "24/11/25 11:10:16 INFO DAGScheduler: ResultStage 4 (showString at NativeMethodAccessorImpl.java:0) finished in 0,019 s\n",
      "24/11/25 11:10:16 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/11/25 11:10:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
      "24/11/25 11:10:16 INFO DAGScheduler: Job 4 finished: showString at NativeMethodAccessorImpl.java:0, took 0,022090 s\n",
      "24/11/25 11:10:16 INFO CodeGenerator: Code generated in 3.228583 ms\n"
     ]
    }
   ],
   "source": [
    "df.select('film_words').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "316d524f-29f0-46f9-82ad-30e5db58dc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('counted', f.size('film_words'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "627c71f9-1f68-44db-aa78-7fd80c06a665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|                film|counted|\n",
      "+--------------------+-------+\n",
      "|You Will Meet a T...|      7|\n",
      "|Tyler Perry's Why...|      7|\n",
      "|Zack and Miri Mak...|      6|\n",
      "|The Curious Case ...|      6|\n",
      "|Miss Pettigrew Li...|      6|\n",
      "|High School Music...|      6|\n",
      "|He's Just Not Tha...|      6|\n",
      "|The Twilight Saga...|      5|\n",
      "|She's Out of My L...|      5|\n",
      "|Sex and the City Two|      5|\n",
      "|  Sex and the City 2|      5|\n",
      "|Nick and Norah's ...|      5|\n",
      "|  Life as We Know It|      5|\n",
      "|I Love You Philli...|      5|\n",
      "|What Happens in V...|      4|\n",
      "|The Time Traveler...|      4|\n",
      "|The Invention of ...|      4|\n",
      "|    Sex and the City|      4|\n",
      "|     P.S. I Love You|      4|\n",
      "|  Over Her Dead Body|      4|\n",
      "+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "<class 'pyspark.sql.types.StructType'>\n",
      "StringType()\n",
      "StringType()\n",
      "StringType()\n",
      "IntegerType()\n",
      "DoubleType()\n",
      "IntegerType()\n",
      "StringType()\n",
      "IntegerType()\n",
      "ArrayType(StringType(), False)\n",
      "IntegerType()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/25 15:16:15 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/11/25 15:16:15 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/11/25 15:16:15 INFO MemoryStore: Block broadcast_41 stored as values in memory (estimated size 353.1 KiB, free 366.0 MiB)\n",
      "24/11/25 15:16:15 INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 35.1 KiB, free 365.9 MiB)\n",
      "24/11/25 15:16:15 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on 10.8.61.21:60155 (size: 35.1 KiB, free: 366.3 MiB)\n",
      "24/11/25 15:16:15 INFO SparkContext: Created broadcast 41 from showString at <unknown>:0\n",
      "24/11/25 15:16:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4199377 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/11/25 15:16:15 INFO SparkContext: Starting job: showString at <unknown>:0\n",
      "24/11/25 15:16:15 INFO DAGScheduler: Got job 21 (showString at <unknown>:0) with 1 output partitions\n",
      "24/11/25 15:16:15 INFO DAGScheduler: Final stage: ResultStage 21 (showString at <unknown>:0)\n",
      "24/11/25 15:16:15 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/11/25 15:16:15 INFO DAGScheduler: Missing parents: List()\n",
      "24/11/25 15:16:15 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[100] at showString at <unknown>:0), which has no missing parents\n",
      "24/11/25 15:16:15 INFO MemoryStore: Block broadcast_42 stored as values in memory (estimated size 17.9 KiB, free 365.9 MiB)\n",
      "24/11/25 15:16:15 INFO MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 8.8 KiB, free 365.9 MiB)\n",
      "24/11/25 15:16:15 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on 10.8.61.21:60155 (size: 8.8 KiB, free: 366.3 MiB)\n",
      "24/11/25 15:16:15 INFO SparkContext: Created broadcast 42 from broadcast at DAGScheduler.scala:1585\n",
      "24/11/25 15:16:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[100] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/11/25 15:16:15 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0\n",
      "24/11/25 15:16:15 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 21) (10.8.61.21, executor driver, partition 0, PROCESS_LOCAL, 9925 bytes) \n",
      "24/11/25 15:16:15 INFO Executor: Running task 0.0 in stage 21.0 (TID 21)\n",
      "24/11/25 15:16:15 INFO FileScanRDD: Reading File path: file:///Users/tuzhbagrigoriy/Downloads/movies.csv, range: 0-5073, partition values: [empty row]\n",
      "24/11/25 15:16:15 INFO Executor: Finished task 0.0 in stage 21.0 (TID 21). 2976 bytes result sent to driver\n",
      "24/11/25 15:16:15 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 21) in 5 ms on 10.8.61.21 (executor driver) (1/1)\n",
      "24/11/25 15:16:15 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool \n",
      "24/11/25 15:16:15 INFO DAGScheduler: ResultStage 21 (showString at <unknown>:0) finished in 0,009 s\n",
      "24/11/25 15:16:15 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/11/25 15:16:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished\n",
      "24/11/25 15:16:15 INFO DAGScheduler: Job 21 finished: showString at <unknown>:0, took 0,010416 s\n"
     ]
    }
   ],
   "source": [
    "df.select(['film', 'counted']).orderBy('counted', ascending=False).show()\n",
    "print(type(df.schema))\n",
    "for i in df.schema.fields:\n",
    "    print(i.dataType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f313a195-c5bb-42df-8f6c-7e3a104d2ca0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4dd951-d319-4ee0-97a6-68788efc9939",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2cd643-db13-408c-8f23-67441b1c994d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018ec73f-596f-405d-8306-391cf29330b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c44e0b-b52e-43f5-b6f6-d1492be3df6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark /\n",
    "    .read / \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b777fa64-90cf-4a42-a08e-1fddeb2f9d51",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (2521887845.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[34], line 8\u001b[0;36m\u001b[0m\n\u001b[0;31m    typesAndData += (\"DecimalType(38,0)\" -> BigDecimal.valueOf(12345678901234567890123456789012345678D))\u001b[0m\n\u001b[0m                                                                                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "// Объявление переменной typesAndData как Map\n",
    "var typesAndData: Map[String, Any] = Map()\n",
    "\n",
    "// Добавляем различные типы данных\n",
    "typesAndData += (\"StringType\" -> \"This_string_is_generated_for_test\")\n",
    "typesAndData += (\"DateType\" -> java.sql.Date.valueOf(java.time.LocalDate.now()))\n",
    "typesAndData += (\"DecimalType(1,0)\" -> BigDecimal.valueOf(0))\n",
    "typesAndData += (\"DecimalType(38,0)\" -> BigDecimal.valueOf(12345678901234567890123456789012345678D))\n",
    "typesAndData += (\"DecimalType(27,6)\" -> BigDecimal.valueOf(123456789123456789123456781.123456D))\n",
    "typesAndData += (\"DecimalType(27,0)\" -> BigDecimal.valueOf(123456789123456789123456781D))\n",
    "typesAndData += (\"DecimalType(38,12)\" -> BigDecimal.valueOf(12345678912345678912345678912345678912.123456789123D))\n",
    "typesAndData += (\"TimestampType\" -> java.sql.Timestamp.valueOf(java.time.LocalDateTime.now()))\n",
    "typesAndData += (\"DoubleType\" -> 1221.11)\n",
    "typesAndData += (\"IntegerType\" -> 5)\n",
    "typesAndData += (\"LongType\" -> 6846637195593252865L)\n",
    "typesAndData += (\"BooleanType\" -> true)\n",
    "typesAndData += (\"ShortType\" -> 10.toShort)\n",
    "typesAndData += (\"ArrayType(LongType,true)\" -> List(4234252643L, 53466456L, 65645645L))\n",
    "\n",
    "// Добавляем массив структур key-value\n",
    "typesAndData += (\"ArrayType(MapType(StringType,StringType),true)\" -> List(\n",
    "  Map(\"key1\" -> \"value1\", \"key2\" -> \"value2\"),   // Структура key-value\n",
    "  Map(\"key3\" -> \"value3\")                         // Еще одна структура key-value\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8088ce3-f7bf-45cd-a717-ac51e0c95346",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
